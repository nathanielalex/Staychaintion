{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Property Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    " <li>There are 4 attributes in the datasets that we use, which are Longitude, Latitude, Room Type, and Minimum Nights.</li>\n",
    " <li>We are manually Data Augmented by combine many datasets to get better datasets for training the model.</li>\n",
    " <li>Searching the best model by evaluate 5 models that suitable for Price Prediction</li>\n",
    " <li>After that we are going to Hyperparameter Tuning the Best model by using GridSearch and Cross Validation</li>\n",
    " <li>For the result we got 3 .pkl file which are the \"best_encoder.pkl\", \"best_scaler.pkl\", and \"best_price_model.pkl\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Model that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma, lalu konversi ke float\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal (`room_type` saja)\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik (latitude, longitude, minimum_nights)\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Daftar Model untuk Dibandingkan\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# 🔹 Step 10: Evaluasi Setiap Model\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"MSE\": round(mse, 2),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"R² Score\": round(r2, 4),\n",
    "        \"Train Time (s)\": round(train_time, 2)\n",
    "    })\n",
    "\n",
    "    print(f\"{model_name} - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}, Time: {train_time:.2f}s\")\n",
    "\n",
    "# 🔹 Step 11: Konversi Hasil ke DataFrame dan Simpan\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"MAE\")  # Sorting berdasarkan akurasi terbaik (MAE terkecil)\n",
    "print(\"\\nModel Comparison Results:\\n\", results_df)\n",
    "\n",
    "# 🔹 Step 12: Simpan Model Terbaik\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(f\"\\n✅ Best model saved: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Random Forest...\n",
    "Random Forest - MAE: 124.04, MSE: 534884.37, RMSE: 731.36, R²: -0.0414, Time: 470.78s\n",
    "Training Gradient Boosting...\n",
    "Gradient Boosting - MAE: 151.09, MSE: 485255.29, RMSE: 696.60, R²: 0.0552, Time: 63.75s\n",
    "Training XGBoost...\n",
    "XGBoost - MAE: 144.64, MSE: 467685.75, RMSE: 683.88, R²: 0.0894, Time: 2.38s\n",
    "Training Linear Regression...\n",
    "Linear Regression - MAE: 158.87, MSE: 508261.84, RMSE: 712.92, R²: 0.0104, Time: 0.11s\n",
    "Training K-Nearest Neighbors...\n",
    "K-Nearest Neighbors - MAE: 135.81, MSE: 492108.54, RMSE: 701.50, R²: 0.0419, Time: 0.90s\n",
    "\n",
    "Model Comparison Results:\n",
    "                  Model     MAE        MSE    RMSE  R² Score  Train Time (s)\n",
    "0        Random Forest  124.04  534884.37  731.36   -0.0414          470.78\n",
    "4  K-Nearest Neighbors  135.81  492108.54  701.50    0.0419            0.90\n",
    "2              XGBoost  144.64  467685.75  683.88    0.0894            2.38\n",
    "1    Gradient Boosting  151.09  485255.29  696.60    0.0552           63.75\n",
    "3    Linear Regression  158.87  508261.84  712.92    0.0104            0.11\n",
    "\n",
    "✅ Best model saved: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights (1)\n",
    "\n",
    "Based on the Model Evaluation,\n",
    "<br><br>\n",
    "✅ MAE (Mean Absolute Error) → The least, the best model\n",
    "\n",
    "The average of the absolute differences between the actual values ​​(y_test) and the predicted values\n",
    "\n",
    "✅ MSE (Mean Squared Error) → The least, the best model\n",
    "\n",
    "The average of the squared differences between the actual and predicted values.\n",
    "\n",
    "✅ RMSE (Root Mean Squared Error) → The least, the best model\n",
    "\n",
    "Akar dari MSE.\n",
    "\n",
    "✅ R² Score → Approach 1 is the best model\n",
    "\n",
    "Measures how well a model explains the variability of the data.\n",
    "\n",
    "The best model based on the evaluation => <b> Random Forest </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan (Tanpa `neighbourhood`)\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma, lalu konversi ke float\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal (`room_type` saja)\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik (`latitude`, `longitude`, `minimum_nights`)\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Hyperparameter Tuning dengan GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],  # Jumlah pohon dalam hutan\n",
    "    \"max_depth\": [10, 20, None],  # Kedalaman maksimal pohon\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum sampel untuk split node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum sampel di tiap leaf node\n",
    "}\n",
    "\n",
    "print(\"🔍 Performing GridSearchCV for Hyperparameter Tuning...\")\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                           param_grid, cv=5, scoring=\"neg_mean_absolute_error\",\n",
    "                           verbose=1, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best Parameters Found: {best_params}\")\n",
    "\n",
    "# 🔹 Step 10: Train Model Terbaik dengan Parameter Optimal\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Step 11: Evaluasi Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n🔹 Model Evaluation Metrics:\")\n",
    "print(f\"📌 Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"📌 Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"📌 Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"📌 R² Score: {r2:.4f}\")\n",
    "print(f\"📌 Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# 🔹 Step 12: Cross Validation untuk Validasi Model\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "\n",
    "print(f\"\\n✅ Cross Validation R² Scores: {cv_scores}\")\n",
    "print(f\"✅ Mean R² Score (Cross Validation): {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# 🔹 Step 13: Simpan Model Terbaik\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✅ Model training complete. Best model saved as 'best_price_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights (2)\n",
    "\n",
    "Because there is issue with CPU/GPU and RAM, I tune the model in Google Colab\n",
    "\n",
    "And finally got the result 3 .pkl file which are the \"best_encoder.pkl\", \"best_scaler.pkl\", and \"best_price_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Hyperparameter Tuning dengan RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [10, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "print(\"🔍 Performing RandomizedSearchCV for Hyperparameter Tuning...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=10, cv=3, scoring=\"neg_mean_absolute_error\",\n",
    "                                   verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(f\"✅ Best Parameters Found: {best_params}\")\n",
    "\n",
    "# 🔹 Step 10: Train Model Terbaik dengan Warm Start\n",
    "best_model = RandomForestRegressor(**best_params, warm_start=True, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Step 11: Evaluasi Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"📌 Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"📌 Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# 🔹 Step 12: Simpan Model Terbaik\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✅ Model training complete. Best model saved as 'best_price_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights (3)\n",
    "\n",
    "Easier and Faster way to tune the Random Forest model and got the best AI Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<ul>\n",
    "    <li>We are using Random Forest to got the AI Model Price Predictions for Staychaintion Properties</li>\n",
    "    <li>We already hyperparameter tuning the Random Forest Model to get the best .pkl file</li>\n",
    "    <li>For the dataset we also combine many datasets and choose 4 attributes that very relevant and greatly influences Price Prediction</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvement\n",
    "\n",
    "<ul>\n",
    "    <li>Using more accurate and reliable datasets</li>\n",
    "    <li>Using more attributes that are influences the Price Prediction for Property</li>\n",
    "    <li>Find better model and tuning it again using the new datasets</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Reference\n",
    "\n",
    "- https://www.kaggle.com/datasets/marcosgarcia75/air-bnb-dataset\n",
    "- https://www.kaggle.com/datasets/vrindakallu/new-york-dataset\n",
    "- https://www.kaggle.com/datasets/kritikseth/us-airbnb-open-data?select=AB_US_2023.csv\n",
    "- https://www.kaggle.com/datasets/thedevastator/airbnbs-nyc-overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
