{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest - MAE: 124.04, MSE: 534884.37, RMSE: 731.36, R²: -0.0414, Time: 470.78s\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - MAE: 151.09, MSE: 485255.29, RMSE: 696.60, R²: 0.0552, Time: 63.75s\n",
      "Training XGBoost...\n",
      "XGBoost - MAE: 144.64, MSE: 467685.75, RMSE: 683.88, R²: 0.0894, Time: 2.38s\n",
      "Training Linear Regression...\n",
      "Linear Regression - MAE: 158.87, MSE: 508261.84, RMSE: 712.92, R²: 0.0104, Time: 0.11s\n",
      "Training K-Nearest Neighbors...\n",
      "K-Nearest Neighbors - MAE: 135.81, MSE: 492108.54, RMSE: 701.50, R²: 0.0419, Time: 0.90s\n",
      "\n",
      "Model Comparison Results:\n",
      "                  Model     MAE        MSE    RMSE  R² Score  Train Time (s)\n",
      "0        Random Forest  124.04  534884.37  731.36   -0.0414          470.78\n",
      "4  K-Nearest Neighbors  135.81  492108.54  701.50    0.0419            0.90\n",
      "2              XGBoost  144.64  467685.75  683.88    0.0894            2.38\n",
      "1    Gradient Boosting  151.09  485255.29  696.60    0.0552           63.75\n",
      "3    Linear Regression  158.87  508261.84  712.92    0.0104            0.11\n",
      "\n",
      "✅ Best model saved: Random Forest\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma, lalu konversi ke float\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal (`room_type` saja)\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik (latitude, longitude, minimum_nights)\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Daftar Model untuk Dibandingkan\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# 🔹 Step 10: Evaluasi Setiap Model\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"MSE\": round(mse, 2),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"R² Score\": round(r2, 4),\n",
    "        \"Train Time (s)\": round(train_time, 2)\n",
    "    })\n",
    "\n",
    "    print(f\"{model_name} - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}, Time: {train_time:.2f}s\")\n",
    "\n",
    "# 🔹 Step 11: Konversi Hasil ke DataFrame dan Simpan\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"MAE\")  # Sorting berdasarkan akurasi terbaik (MAE terkecil)\n",
    "print(\"\\nModel Comparison Results:\\n\", results_df)\n",
    "\n",
    "# 🔹 Step 12: Simpan Model Terbaik\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(f\"\\n✅ Best model saved: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Random Forest...\n",
    "Random Forest - MAE: 124.04, MSE: 534884.37, RMSE: 731.36, R²: -0.0414, Time: 470.78s\n",
    "Training Gradient Boosting...\n",
    "Gradient Boosting - MAE: 151.09, MSE: 485255.29, RMSE: 696.60, R²: 0.0552, Time: 63.75s\n",
    "Training XGBoost...\n",
    "XGBoost - MAE: 144.64, MSE: 467685.75, RMSE: 683.88, R²: 0.0894, Time: 2.38s\n",
    "Training Linear Regression...\n",
    "Linear Regression - MAE: 158.87, MSE: 508261.84, RMSE: 712.92, R²: 0.0104, Time: 0.11s\n",
    "Training K-Nearest Neighbors...\n",
    "K-Nearest Neighbors - MAE: 135.81, MSE: 492108.54, RMSE: 701.50, R²: 0.0419, Time: 0.90s\n",
    "\n",
    "Model Comparison Results:\n",
    "                  Model     MAE        MSE    RMSE  R² Score  Train Time (s)\n",
    "0        Random Forest  124.04  534884.37  731.36   -0.0414          470.78\n",
    "4  K-Nearest Neighbors  135.81  492108.54  701.50    0.0419            0.90\n",
    "2              XGBoost  144.64  467685.75  683.88    0.0894            2.38\n",
    "1    Gradient Boosting  151.09  485255.29  696.60    0.0552           63.75\n",
    "3    Linear Regression  158.87  508261.84  712.92    0.0104            0.11\n",
    "\n",
    "✅ Best model saved: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Performing GridSearchCV for Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     58\u001b[39m grid_search = GridSearchCV(RandomForestRegressor(random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     59\u001b[39m                            param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m\"\u001b[39m\u001b[33mneg_mean_absolute_error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m                            verbose=\u001b[32m1\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m     62\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m train_time = time.time() - start_time\n\u001b[32m     66\u001b[39m best_params = grid_search.best_params_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/CodeFest/Staychaintion/ai-model/venv/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan (Tanpa `neighbourhood`)\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma, lalu konversi ke float\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal (`room_type` saja)\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik (`latitude`, `longitude`, `minimum_nights`)\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Hyperparameter Tuning dengan GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],  # Jumlah pohon dalam hutan\n",
    "    \"max_depth\": [10, 20, None],  # Kedalaman maksimal pohon\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum sampel untuk split node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum sampel di tiap leaf node\n",
    "}\n",
    "\n",
    "print(\"🔍 Performing GridSearchCV for Hyperparameter Tuning...\")\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                           param_grid, cv=5, scoring=\"neg_mean_absolute_error\",\n",
    "                           verbose=1, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best Parameters Found: {best_params}\")\n",
    "\n",
    "# 🔹 Step 10: Train Model Terbaik dengan Parameter Optimal\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Step 11: Evaluasi Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n🔹 Model Evaluation Metrics:\")\n",
    "print(f\"📌 Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"📌 Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"📌 Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"📌 R² Score: {r2:.4f}\")\n",
    "print(f\"📌 Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# 🔹 Step 12: Cross Validation untuk Validasi Model\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "\n",
    "print(f\"\\n✅ Cross Validation R² Scores: {cv_scores}\")\n",
    "print(f\"✅ Mean R² Score (Cross Validation): {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# 🔹 Step 13: Simpan Model Terbaik\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✅ Model training complete. Best model saved as 'best_price_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 🔹 Step 1: Gabungkan 6 Dataset dengan Tipe Data yang Jelas\n",
    "file_paths = [\n",
    "    \"Datasets/data.csv\", \"Datasets/data1.csv\", \"Datasets/data2.csv\",\n",
    "    \"Datasets/data3.csv\", \"Datasets/data4.csv\", \"Datasets/data5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(file, dtype={\"room_type\": str}, low_memory=False) for file in file_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔹 Step 2: Pilih Kolom yang Dibutuhkan\n",
    "selected_features = [\"latitude\", \"longitude\", \"minimum_nights\", \"room_type\", \"price\"]\n",
    "data = data[selected_features]\n",
    "\n",
    "# 🔹 Step 3: Bersihkan `price` dari simbol \"$\" dan koma\n",
    "data[\"price\"] = data[\"price\"].astype(str).str.replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# 🔹 Step 4: Handle Missing Values\n",
    "data.fillna({\n",
    "    \"minimum_nights\": data[\"minimum_nights\"].median(),\n",
    "    \"room_type\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 🔹 Step 5: Encode Kategorikal\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[[\"room_type\"]])\n",
    "\n",
    "# 🔹 Step 6: Standarisasi Fitur Numerik\n",
    "numerical_features = [\"latitude\", \"longitude\", \"minimum_nights\"]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# 🔹 Step 7: Gabungkan Semua Fitur\n",
    "X = np.hstack((encoded_features, scaled_features))\n",
    "y = data[\"price\"]\n",
    "\n",
    "# 🔹 Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Step 9: Hyperparameter Tuning dengan RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [10, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "print(\"🔍 Performing RandomizedSearchCV for Hyperparameter Tuning...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=10, cv=3, scoring=\"neg_mean_absolute_error\",\n",
    "                                   verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(f\"✅ Best Parameters Found: {best_params}\")\n",
    "\n",
    "# 🔹 Step 10: Train Model Terbaik dengan Warm Start\n",
    "best_model = RandomForestRegressor(**best_params, warm_start=True, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Step 11: Evaluasi Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"📌 Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"📌 Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# 🔹 Step 12: Simpan Model Terbaik\n",
    "joblib.dump(best_model, \"best_price_model.pkl\")\n",
    "joblib.dump(encoder, \"best_encoder.pkl\")\n",
    "joblib.dump(scaler, \"best_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✅ Model training complete. Best model saved as 'best_price_model.pkl'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
